# -*- coding: utf-8 -*-
"""run_trained_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15D4zIVt6aPVlC6L8ZpgZXzwZGujnYD6s

# Load data
"""

import os
import librosa
import numpy as np
!pip install ffmpeg
import ffmpeg
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from warnings import filterwarnings
filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive/')

def extract_features(file_path, n_mfcc=20):
    audio, sr = librosa.load(file_path, sr=None)
    n_fft = min(1024, len(audio))

    # Extract MFCC features
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft)
    mfccs_mean = np.mean(mfccs, axis=1)

    # Add delta and delta-delta MFCCs
    delta_mfcc = librosa.feature.delta(mfccs)
    delta_delta_mfcc = librosa.feature.delta(mfccs, order=2)

    delta_mfcc_mean = np.mean(delta_mfcc, axis=1)
    delta_delta_mfcc_mean = np.mean(delta_delta_mfcc, axis=1)

    # Extract Tonnetz features
    harmonic = librosa.effects.harmonic(audio)
    tonnetz = librosa.feature.tonnetz(y=harmonic, sr=sr)
    tonnetz_mean = np.mean(tonnetz, axis=1)

    # Combine features
    features = np.concatenate((mfccs_mean, delta_mfcc_mean, delta_delta_mfcc_mean, tonnetz_mean))
    return features


def load_dataset(audio_dir):
    features = []
    y = []

    for filename in os.listdir(audio_dir):
        if filename.endswith('.wav'):
            file_path = os.path.join(audio_dir, filename)
            try:
                feature_vector = extract_features(file_path)
                features.append(feature_vector)
                y.append(filename)
            except ValueError as e:
                print(f"Skipping file {file_path}: {e}")
            except Exception as e:
                print(f"Error processing {file_path}: {e}")

    return np.array(features)

def load_sample_dataset(sample_dir):
  # Set the base directory path
  base_dir = sample_dir
  X_sample = []
  Y_sample = []
  # Loop through subfolders
  for idx, subfolder in enumerate(os.listdir(base_dir)):
      subfolder_path = os.path.join(base_dir, subfolder)

      # Check if it's a directory
      if os.path.isdir(subfolder_path):
          x = load_dataset(subfolder_path)
          y = np.array([CLASS_TO_LABEL[subfolder]] * len(x))

          X_sample.append(x)
          Y_sample.append(y)

  X_sample = np.concatenate(X_sample, 0)
  Y_sample = np.concatenate(Y_sample, 0)
  return X_sample, Y_sample

def load_real_dataset(sample_dir):
  # Set the base directory path
  base_dir = sample_dir
  X_real = []
  Y_real = []
  # Loop through subfolders
  for idx, subfolder in enumerate(os.listdir(base_dir)):
      subfolder_path = os.path.join(base_dir, subfolder)

      # Check if it's a directory
      if os.path.isdir(subfolder_path):
          x = load_dataset(subfolder_path)
          y = np.array([CLASS_TO_LABEL[subfolder]] * len(x))

          X_real.append(x)
          Y_real.append(y)

  X_real = np.concatenate(X_real, 0)
  Y_real = np.concatenate(Y_real, 0)
  return X_real, Y_real

"""# Feature selection
only chi-best is useful, feature 66 to 30
"""

def feature_selection(X_sample,Y_sample,X_real,Y_real ):
  #Standarlization
  # Initialize the scaler
  scaler = StandardScaler()
  # Fit the scaler on X_sample and transform both datasets
  X_sample_scaled = scaler.fit_transform(X_sample)
  X_real_scaled = scaler.transform(X_real)

  #Select chi-best

  #  Scale the training set (X_real)
  scaler = MinMaxScaler()
  X_real_scaled = scaler.fit_transform(X_real)

  # Apply feature selection on the training set
  selector = SelectKBest(chi2, k=30)
  X_real_selected = selector.fit_transform(X_real_scaled, Y_real)


  # Same scaler applied
  X_sample_scaled = scaler.transform(X_sample)
  X_sample_selected = selector.transform(X_sample_scaled)

  return X_real_selected,X_sample_selected

"""# Model

## Logitisc + SVM + RF - 0.91
"""

def run_trained_model(X_real_selected,Y_real,X_sample_selected,Y_sample):
  # Step 1: Train-test split
  X_train, X_test, y_train, y_test = train_test_split(X_real_selected, Y_real, test_size=0.2, random_state=42)
  X_val = X_sample_selected
  y_val = Y_sample

  # Step 2: Standardize the features
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_val_scaled = scaler.transform(X_val)
  X_test_scaled = scaler.transform(X_test)

  # Step 3: Initialize base models (Logistic Regression, SVM, and Random Forest)
  logreg = LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced')  # Weighted for class imbalance
  svm = SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', probability=True)  # Enable probability estimation
  rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')

  # Step 4: Train the base models
  logreg.fit(X_train_scaled, y_train)
  svm.fit(X_train_scaled, y_train)
  rf.fit(X_train_scaled, y_train)

  combined_proba = download_model_weights(X_val_scaled,logreg,svm,rf)

  # Step 8: Make final predictions based on the highest probability
  y_pred_val = np.argmax(combined_proba, axis=1)
  return y_pred_val

def download_model_weights(X_val_scaled,logreg,svm,rf):
  # Step 5: Get predicted probabilities from all models
  logreg_proba = logreg.predict_proba(X_val_scaled)  # Probabilities from Logistic Regression
  svm_proba = svm.predict_proba(X_val_scaled)  # Probabilities from SVM
  rf_proba = rf.predict_proba(X_val_scaled)  # Probabilities from Random Forest

  # Step 6: Apply class-specific weighting
  # For class 0 and 6, give more weight to Logistic Regression
  # For class 2 and 5, give more weight to SVM
  # For class 1, give more weight to Random Forest
  logreg_weighted = logreg_proba.copy()
  svm_weighted = svm_proba.copy()
  rf_weighted = rf_proba.copy()

  logreg_weighted[:,[1]] *= 1.5
  rf_weighted[:, [4]] *= 1.2
  rf_weighted[:, [0,1,2,3,5,6]] *= 0.1

  # Step 7: Combine the weighted probabilities
  combined_proba = (logreg_weighted + svm_weighted + rf_weighted) / 2  # Average the weighted probabilities

  return combined_proba

CLASS_TO_LABEL = {
    'water': 0,
    'table': 1,
    'sofa': 2,
    'railing': 3,
    'glass': 4,
    'blackboard': 5,
    'ben': 6,
}

LABEL_TO_CLASS = {v: k for k, v in CLASS_TO_LABEL.items()}
sample_dir = '/content/drive/Shareddrives/CIS5190_Final_Project/Sample_Evaluation_Data'
real_dir = '/content/drive/Shareddrives/CIS5190_Final_Project/Train_Data_Old'

X_sample, Y_sample = load_sample_dataset(sample_dir)
X_real, Y_real = load_real_dataset(real_dir)
X_real_selected,X_sample_selected = feature_selection(X_sample,Y_sample,X_real,Y_real)
Y_pred  = run_trained_model(X_real_selected,Y_real,X_sample_selected,Y_sample)
accuracy = accuracy_score(Y_sample, Y_pred)
print(f"Model Accuracy on sample data: {accuracy:.2f}")

for i in range(len(Y_sample)):
  if Y_sample[i] != Y_pred[i]:
    print(f'Correct {(Y_sample[i] == Y_pred[i])}, Pred {Y_pred[i]}, Label: {Y_sample[i]}')