# -*- coding: utf-8 -*-
"""run_trained_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13axYBlSSNuVQX669K9wIABSmo1jre97l

# Skeleton code


Here is some skeleton code to get you started. We will provide you with the dataset loading and evaluation code. You will need to design a model that takes in the .wav files, and outputs a label.

## Class Label
- Ben Franklin statue - Google Map "Ben on the Bench"
- Blackboard (not white board) - Levine 4th floor bump space
- Table - Levine 4th floor near window
- Glass window - Levine 4th floor
- Handrail - Levine 4th floor
- Water fountain - Levine 4th floor
- Sofa - Levine 4th floor bump space


### 1. Multi-Class Classification with Logistic Regression
"""

import os
import librosa
import numpy as np

from google.colab import drive
drive.mount('/content/drive/')

eval_folder = "/content/drive/Shareddrives/(TA Team Drive) CIS 4190 5190 Applied Machine Learning Fall 2024/[Project] Sound-based Material Classification/Data"
# eval_folder = '/content/drive/Shareddrives/CIS5190_Final_Project/Sample_Evaluation_Data'
os.chdir(eval_folder)
print([d for d in os.listdir(eval_folder)])

CLASS_TO_LABEL = {
    'water': 0,
    'table': 1,
    'sofa': 2,
    'railing': 3,
    'glass': 4,
    'blackboard': 5,
    'ben': 6,
}

LABEL_TO_CLASS = {v: k for k, v in CLASS_TO_LABEL.items()}

# Set the base directory path
base_dir = eval_folder
X = []
Y = []
# Loop through subfolders
for idx, class_folder in enumerate(os.listdir(base_dir)):
    class_folder_path = os.path.join(base_dir, class_folder)

    # Check if it's a directory
    if os.path.isdir(class_folder_path):
        y = CLASS_TO_LABEL[class_folder]
        for sample in os.listdir(class_folder_path):
            file_path = os.path.join(class_folder_path, sample)
            X.append(file_path)
            Y.append(y)
X = np.array(X)
Y = np.array(Y)

print(X.shape)
print(Y.shape)
print(X[::10])
print(Y[::10])

"""# Model definition
When we evaluate your model, we expect that you provide your model weights and model setup in this codeblock.

Make sure your model weights are public!
"""

import numpy as np
import librosa
import torch
import torch.nn as nn
import torch.nn.functional as F

def run_trained_model(X):
  def extract_features(file_paths, max_pad_len=128):
      features = []
      for file_path in file_paths:
          try:
              audio, sr = librosa.load(file_path, sr=22050, duration=5.0)
              audio = augment_audio(audio, sr)
          except Exception as e:
              print(f"Error loading audio file {file_path}: {e}")
              audio = np.zeros(22050 * 5)
              sr = 22050

          mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)
          spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)
          chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
          mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)

          combined_feature = np.concatenate([
              np.mean(mfccs, axis=1), np.std(mfccs, axis=1),
              np.mean(spectral_contrast, axis=1), np.std(spectral_contrast, axis=1),
              np.mean(chroma, axis=1), np.std(chroma, axis=1),
              np.mean(mel_spectrogram, axis=1), np.std(mel_spectrogram, axis=1)
          ])

          features.append(combined_feature)

      return np.array(features)

  def augment_audio(audio, sr):
      rate = np.random.uniform(0.8, 1.2)
      audio_stretched = librosa.effects.time_stretch(audio, rate=rate)

      n_steps = np.random.randint(-3, 4)
      audio_pitched = librosa.effects.pitch_shift(audio_stretched, sr=sr, n_steps=n_steps)

      noise_factor = np.random.uniform(0, 0.05)
      noise = np.random.randn(len(audio_pitched))
      audio_noisy = audio_pitched + noise_factor * noise

      return audio_noisy

  class EnhancedAudioClassificationModel(nn.Module):
      def __init__(self, input_dim, num_classes=7):
          super(EnhancedAudioClassificationModel, self).__init__()

          self.bn_input = nn.BatchNorm1d(input_dim)

          self.fc1 = nn.Linear(input_dim, 256)
          self.bn1 = nn.BatchNorm1d(256)
          self.dropout1 = nn.Dropout(0.3)

          self.fc2 = nn.Linear(256, 128)
          self.bn2 = nn.BatchNorm1d(128)
          self.dropout2 = nn.Dropout(0.4)

          self.fc3 = nn.Linear(128, 64)
          self.bn3 = nn.BatchNorm1d(64)
          self.dropout3 = nn.Dropout(0.5)

          self.fc_out = nn.Linear(64, num_classes)

      def forward(self, x):
          x = self.bn_input(x)

          x = F.leaky_relu(self.fc1(x))
          x = self.bn1(x)
          x = self.dropout1(x)

          x = F.leaky_relu(self.fc2(x))
          x = self.bn2(x)
          x = self.dropout2(x)

          x = F.leaky_relu(self.fc3(x))
          x = self.bn3(x)
          x = self.dropout3(x)

          x = self.fc_out(x)
          return x

  def download_model_weights():
      import gdown
      url = 'https://drive.google.com/file/d/1mw1bxK5oMxLUg4I9JNXZMzx3zrKxr21D/view?usp=drive_link'
      output = "my_weights.pth"  # Changed to .pth for PyTorch weights
      gdown.download(url, output, fuzzy=True)
      return output

  def predict_audio_classes(X):
        """
        Predict classes for input audio files

        Args:
            X (list): List of file paths to WAV files

        Returns:
            np.ndarray: Predicted class labels
        """
        # Extract features
        X_features = extract_features(X)

        # Convert features to PyTorch tensor
        X_tensor = torch.FloatTensor(X_features)

        # Initialize model with the same architecture
        model = EnhancedAudioClassificationModel(input_dim=X_features.shape[1], num_classes=7)


        # Load pre-trained weights
        weight_path = download_model_weights()

        # Load weights correctly
        try:
            # Try loading as a dictionary of state_dict
            weights_dict = torch.load(weight_path)
            model.load_state_dict(weights_dict)
        except Exception:
            # If that fails, try loading as numpy array and converting
            weights = np.load(weight_path, allow_pickle=True)

            # Create a state dictionary that matches the model's structure
            state_dict = model.state_dict()
            for name, param in state_dict.items():
                if name in weights:
                    state_dict[name] = torch.FloatTensor(weights[name])

            model.load_state_dict(state_dict)

        # Set model to evaluation mode
        model.eval()

        # Get predictions
        with torch.no_grad():
            outputs = model(X_tensor)
            _, predictions = torch.max(outputs, 1)


        return predictions.numpy()

  # Use the predict_audio_classes function for predictions
  predictions = predict_audio_classes(X)

  assert predictions.shape == (len(X),)
  return predictions

# Test code
from sklearn.metrics import accuracy_score

Y_pred = run_trained_model(X)
accuracy = accuracy_score(Y, Y_pred)
print(f"Model Accuracy on unseen data: {accuracy:.2f}")

for i in range(len(Y)):
  if Y[i] != Y_pred[i]:
    print(f'Correct {(Y[i] == Y_pred[i])}, Pred {Y_pred[i]}, Label: {Y[i]}')